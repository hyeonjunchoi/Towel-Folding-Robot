{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1x8ls0zKg_IbRBBic325NBfMDnMdDxQhj","authorship_tag":"ABX9TyPzDAGivzsomvapNqAQsw0e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"bEYSKT9ZnHFx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732673907063,"user_tz":-540,"elapsed":26868,"user":{"displayName":"최현준","userId":"07510304324805911613"}},"outputId":"dfa95291-558a-4a77-db11-d18673779a63"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of pictures: 6917\n","\n","Number of different labels: 9\n","\n","Labels: ['cat' 'motorbike' 'flower' 'car' 'person' 'dog' 'airplane' 'fruit'\n"," 'towel']\n"]}],"source":["import os.path\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import seaborn as sns\n","import time\n","\n","from pathlib import Path\n","from tqdm import tqdm\n","from time import perf_counter\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics    import classification_report, accuracy_score\n","from IPython.display    import Markdown, display\n","from keras.src.legacy.preprocessing.image import ImageDataGenerator\n","\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n","\n","dir_ = Path(r'/content/drive/MyDrive/ImageFolder')\n","filepaths = list(dir_.glob(r'**/*.jpg'))\n","def proc_img(filepath):\n","    \"\"\"\n","   \t\t이미지데이터의 경로와 label데이터로 데이터프레임 만들기\n","    \"\"\"\n","\n","    labels = [str(filepath[i]).split(os.path.sep)[-2] \\\n","              for i in range(len(filepath))]\n","\n","    filepath = pd.Series(filepath, name='Filepath').astype(str)\n","    labels = pd.Series(labels, name='Label')\n","\n","    #경로와 라벨 concatenate\n","    df = pd.concat([filepath, labels], axis=1)\n","\n","    #index 재설정\n","    df = df.sample(frac=1, random_state=0).reset_index(drop = True)\n","\n","    return df\n","\n","df = proc_img(filepaths)\n","df.head(5)\n","\n","print(f'Number of pictures: {df.shape[0]}\\n')\n","print(f'Number of different labels: {len(df.Label.unique())}\\n')\n","print(f'Labels: {df.Label.unique()}')"]},{"cell_type":"code","source":["def create_gen():\n","    # 생성기 및 데이터 증강으로 이미지 로드\n","    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n","        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n","        validation_split=0.1\n","    )\n","\n","    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n","        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n","    )\n","\n","    train_images = train_generator.flow_from_dataframe(\n","        dataframe=train_df,\n","        x_col='Filepath', # 파일위치 열이름\n","        y_col='Label', # 클래스 열이름\n","        target_size=(224, 224), # 이미지 사이즈\n","        color_mode='rgb', # 이미지 채널수\n","        class_mode='categorical', # Y값(Label값)\n","        batch_size=32,\n","        shuffle=True, # 데이터를 섞을지 여부\n","        seed=0,\n","        subset='training', # train 인지 val인지 설정\n","        rotation_range=30, # 회전제한 각도 30도\n","        zoom_range=0.15, # 확대 축소 15%\n","        width_shift_range=0.2, # 좌우이동 20%\n","        height_shift_range=0.2, # 상하이동 20%\n","        shear_range=0.15, # 반시계방햐의 각도\n","        horizontal_flip=True, # 좌우 반전 True\n","        fill_mode=\"nearest\"\n","        # 이미지 변경시 보완 방법 (constant, nearest, reflect, wrap) 4개 존재\n","    )\n","\n","    val_images = train_generator.flow_from_dataframe(\n","        dataframe=train_df,\n","        x_col='Filepath',\n","        y_col='Label',\n","        target_size=(224, 224),\n","        color_mode='rgb',\n","        class_mode='categorical',\n","        batch_size=32,\n","        shuffle=True,\n","        seed=0,\n","        subset='validation',\n","        rotation_range=30,\n","        zoom_range=0.15,\n","        width_shift_range=0.2,\n","        height_shift_range=0.2,\n","        shear_range=0.15,\n","        horizontal_flip=True,\n","        fill_mode=\"nearest\"\n","    )\n","\n","    test_images = test_generator.flow_from_dataframe(\n","        dataframe=test_df,\n","        x_col='Filepath',\n","        y_col='Label',\n","        target_size=(224, 224),\n","        color_mode='rgb',\n","        class_mode='categorical',\n","        batch_size=32,\n","        shuffle=False\n","    )\n","\n","    return train_generator,test_generator,train_images,val_images,test_images"],"metadata":{"id":"OgqMC6NP1xqo","executionInfo":{"status":"ok","timestamp":1732673986657,"user_tz":-540,"elapsed":2,"user":{"displayName":"최현준","userId":"07510304324805911613"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def get_model(model):\n","# Load the pretained model\n","    kwargs =    {'input_shape':(224, 224, 3),\n","                'include_top':False,\n","                'weights':'imagenet',\n","                'pooling':'avg'}\n","\n","    pretrained_model = model(**kwargs)\n","    #model(**kwargs)는 kwargs 내부의 인수가 pretrained_model에 대응되어 들어간다 라는 것을 의미\n","    #위의 line은 input shape를 정의해준다.\n","    pretrained_model.trainable = False # 레이어를 동결 시켜서 훈련중 손실을 최소화 한다.\n","\n","    inputs = pretrained_model.input\n","    #pretrained_model의 input shape에 맞는 input이 들어간다.\n","\n","    x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n","    x = tf.keras.layers.Dense(128, activation='relu')(x)\n","\n","    outputs = tf.keras.layers.Dense(9, activation='softmax')(x)\n","    # 라벨 개수가 8개이기 때문에 Dencs도 8로 설정\n","    # 위의 과정을 통해, outputs이 결정된다.\n","    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","\n","    model.compile(\n","        optimizer='adam',\n","        loss='categorical_crossentropy',\n","        metrics=['accuracy']\n","    )\n","    #model.compile을 통해, Loss가 반영되고, 학습이 진행된다.\n","\n","    return model\n"],"metadata":{"id":"2hC9dNfW15fN","executionInfo":{"status":"ok","timestamp":1732674001278,"user_tz":-540,"elapsed":292,"user":{"displayName":"최현준","userId":"07510304324805911613"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#ResNet으로 실험진행\n","\n","\n","train_df,test_df = train_test_split(df, test_size=0.1, random_state=0)\n","train_generator,test_generator,train_images,val_images,test_images=create_gen()\n","\n","model = get_model(tf.keras.applications.ResNet152V2)\n","history = model.fit(train_images,validation_data=val_images,epochs=5)\n","\n","pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\n","plt.title(\"Accuracy\")\n","plt.show()\n","\n","pd.DataFrame(history.history)[['loss','val_loss']].plot()\n","plt.title(\"Loss\")\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bOLhV9q019eJ","outputId":"da617c5a-8c70-4f36-f86f-681769ee0c9a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 5603 validated image filenames belonging to 9 classes.\n","Found 622 validated image filenames belonging to 9 classes.\n","Found 692 validated image filenames belonging to 9 classes.\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m234545216/234545216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n","Epoch 1/5\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m 45/176\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31:12\u001b[0m 14s/step - accuracy: 0.8439 - loss: 0.5866"]}]},{"cell_type":"code","source":["# Predict the label of the test_images\n","pred = model.predict(test_images)\n","pred = np.argmax(pred,axis=1)\n","\n","# Map the label\n","labels = (train_images.class_indices)\n","labels = dict((v,k) for k,v in labels.items())\n","pred = [labels[k] for k in pred]\n","\n","def printmd(string):\n","    # Print with Markdowns\n","    display(Markdown(string))\n","\n","y_test = list(test_df.Label)\n","acc = accuracy_score(y_test,pred)\n","printmd(f'# Accuracy on the test set: {acc * 100:.2f}%')"],"metadata":{"id":"QTQeTBGS2RNS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import display, Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","from PIL import Image\n","\n","def take_photo(filename='photo.jpg', quality=0.8):\n","  js = Javascript('''\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    ''')\n","  display(js)\n","  data = eval_js('takePhoto({})'.format(quality))\n","  binary = b64decode(data.split(',')[1])\n","  with open(filename, 'wb') as f:\n","    f.write(binary)\n","  return filename"],"metadata":{"id":"U1l7Oogj2UME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_to_png(jpeg_filename, png_filename):\n","    # Open the JPEG image\n","    with Image.open(jpeg_filename) as img:\n","        # Convert and save as PNG\n","        img.save(png_filename, 'PNG')\n","    return png_filename\n","\n","jpeg_photo = take_photo()\n","png_photo = convert_to_png(jpeg_photo, 'photo.png')\n","\n","from IPython.display import Image\n","try:\n","  filename = take_photo()\n","  print('Saved to {}'.format(filename))\n","\n","  # Show the image which was just taken.\n","  display(Image(filename))\n","except Exception as err:\n","  # Errors will be thrown if the user does not have a webcam or if they do not\n","  # grant the page permission to access it.\n","  print(str(err))\n","\n","def capture_image_from_camera():\n","    cap = cv2.VideoCapture(0)  # Open the first camera device\n","    if not cap.isOpened():\n","        raise Exception(\"Could not open video device\")\n","\n","    ret, frame = cap.read()  # Capture a single frame\n","    if not ret:\n","        raise Exception(\"Could not read frame from camera\")\n","\n","    cap.release()\n","    cv2.destroyAllWindows()\n","    return frame\n","\n","IMAGE_SIZE = (224, 224)\n","\n","timeout = 3 * 60\n","\n","start_time = time.time()\n","\n","while True:\n","\n","  camera_image = capture_image_from_camera()\n","\n","  camera_image = cv2.resize(camera_image, IMAGE_SIZE)\n","  camera_image = camera_image.reshape((1, camera_image.shape[0], camera_image.shape[1], camera_image.shape[2]))\n","  camera_image = preprocess_input(camera_image)\n","\n","  pred = model.predict(camera_image)\n","  pred = np.argmax(pred,axis=1)\n","  labels = (train_images.class_indices)\n","  labels = dict((v,k) for k,v in labels.items())\n","  pred = [labels[k] for k in pred]\n","  if pred == 'towel':\n","    print('Its towel')\n","    break\n","\n","  elapsed_time = time.time() - start_time\n","\n","  if elapsed_time >= timeout:\n","    print('Time out')\n","    break\n","\n","  time.sleep(1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"EGwHKeRP2eRq","executionInfo":{"status":"error","timestamp":1732279736840,"user_tz":-540,"elapsed":363,"user":{"displayName":"최현준","userId":"07510304324805911613"}},"outputId":"b0167327-6950-4da6-cc59-6720dc9e0c68"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'take_photo' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-a1bfb0b2577e>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpng_filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mjpeg_photo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_photo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpng_photo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjpeg_photo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'photo.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'take_photo' is not defined"]}]}]}